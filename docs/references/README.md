# References 

## Publications
* Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need." Advances in neural information processing systems 30 (2017).     
	* https://scholar.google.com/scholar?cites=2960712678066186980&as_sdt=2005&sciodt=0,5&hl=en
	* https://arxiv.org/abs/1706.03762 

* Azad, Reza, Amirhossein Kazerouni, Moein Heidari, Ehsan Khodapanah Aghdam, Amirali Molaei, Yiwei Jia, Abin Jose, Rijo Roy, and Dorit Merhof. "Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review." arXiv preprint arXiv:2301.03505 (2023).     
	* https://github.com/mindflow-institue/Awesome-Transformer
	* https://scholar.google.com/scholar?cites=4381569612155749359&as_sdt=2005&sciodt=0,5&hl=en
	* https://arxiv.org/abs/2301.03505 

## Blogs
* Why multi-head self attention works: math, intuitions and 10+1 hidden insights. Nikolas Adaloglouon2021-03-25 
	* https://theaisummer.com/self-attention/

* "TensorFlow, Keras and deep learning, without a PhD" updated on 25 June 2021
	* https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist#0


* Einsum is all you need! > Einstein Summation in Deep Learning 
	* https://rockt.github.io/2018/04/30/einsum

* "Transformers Explained Visually (Part 3): Multi-head Attention, deep dive" by `Ketan Doshi` on Jan 17,2023
	* https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853

* "What Exactly Is Happening Inside the Transformer" by `Huangwei Wieniawska` on Oct 4, 2020
	* https://medium.com/swlh/what-exactly-is-happening-inside-the-transformer-b7f713d7aded

## Repositories 
* https://github.com/The-AI-Summer/self-attention-cv 
* Transformers in Time Series > https://github.com/qingsongedu/time-series-transformers-review

## Courses
* CS25: Transformers United V2 -- Winter 2023
	* https://web.stanford.edu/class/cs25/
		* CS25 I Stanford Seminar - Transformers United 2023: Introduction to Transformers w/ Andrej Karpathy
		* https://www.youtube.com/watch?v=XfpMkf4rD6E 

## Video tutorials
| Title and author  | Video link | Publication date | Language |
| --- | --- | --- | --- | 
| An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained) by Yannic Kilcher | https://www.youtube.com/watch?v=TrdevFK_am4 | 4 Oct 2020 | English |
| Attention Is All You Need by Yannic Kilcher | https://www.youtube.com/watch?v=iDulhoQ2pro | 28 Nov 2017 | English |
| Let's build GPT: from scratch, in code, spelled out. by Andrej Karpathy | https://www.youtube.com/watch?v=kCc8FmEb1nY | 17 Jan 2023 | English |
| Attention is all you need (Transformer) - Model explanation (including math), Inference and Training by Umar Jamil  | https://www.youtube.com/watch?v=hjesn5pCEYc | 11 May 2023 | English |
| Attention is all You need. ¿Cómo aprenden realmente las máquinas? por Jesús Conde | https://www.youtube.com/watch?v=JtT_qpZ5lsk | 6 de Mayo 2023 | Español |
|  |  |  | 

